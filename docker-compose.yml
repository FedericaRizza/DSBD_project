version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    container_name: zookeeper
      
  prometheus_data:
    image: mysql
    restart: always
    ports:
      - 3307:3306 #non serve esporre il db all'esterno, solo i microservizi devono poter collegarsi, fare expose
    environment:
      MYSQL_DATABASE: prometheus_data
      MYSQL_USER: user
      MYSQL_PASSWORD: password
      MYSQL_ROOT_PASSWORD: password
    volumes:
      - ./DataStorage/database.sql:/docker-entrypoint-initdb.d/database.sql
      - db_data:/var/lib/mysql
    container_name: db

  

  broker_kafka:
    image: confluentinc/cp-kafka:latest
    ports:
      - 29092:29092 #non serve se tutti i container sono nella stessa rete, fare expose(forse)
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker_kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    container_name: broker_kafka
  
  init_kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - broker_kafka
    entrypoint: ['/bin/sh', '-c']
    command: |
      "
      kafka-topics --bootstrap-server broker_kafka:9092 --list

      echo -e 'creazione del topic...'
      kafka-topics --create --replication-factor 1 --partitions 1 --topic prometheusdata --bootstrap-server broker_kafka:9092 --if-not-exists

      kafka-topics --bootstrap-server broker_kafka:9092 --list
      "
    tty: true #altrimenti esce subito, Ã¨ come scrivere -t, insieme a stdin_open per avere una shell interattiva
    stdin_open: true
  
  etl_datapipeline:
    build: ./ETLdatapipeline #cambiare con build: ./ETLdatapipeline
    depends_on:
      - broker_kafka
    #volumes: #togliere?
    #  - "./ETLdatapipeline:/file"
    #command: ["/file/ETLDataPipeline.py"] #basta nel dockerfile?
    tty: true
    stdin_open: true
    container_name: etl_datapipeline
  
  datastorage:
    build: ./DataStorage #cambiare con build: ./DataStorage
    depends_on:
      - prometheus_data
      - broker_kafka
    #volumes: #togliere?
    #  - "./DataStorage:/file"
    #command: ["/file/DataStorage.py"] #aggiungere COPY DataStorage.py . nel dockerfile?
    tty: true
    stdin_open: true
    stop_signal: SIGINT
    container_name: datastorage

  dataretrieval:
    build: ./data-retrieval #cambiare con build: ./data-retrieval
    depends_on:
      - prometheus_data
    ports:
      - 80:5000
    #stop_signal: SIGINT
    #volumes: #togliere?
    #- "./data-retrieval:/usr/src/app"
    #command: ["/file/DataRetrieval.py"]
    tty: true
    stdin_open: true
    container_name: dataretrieval

  slamanager:
    build: ./SLAmanager
    depends_on:
      - prometheus_data
    ports: #ports espone la porta del container all'host, expose solo tra i container della rete
      - 50051:50051
    tty: true
    stdin_open: true
    container_name: slamanager

volumes:
  db_data:

            
            
            